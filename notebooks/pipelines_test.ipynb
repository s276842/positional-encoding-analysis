{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/marquez/positional-encoding-analysis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BartTokenizer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import ElectraTokenizer\n",
    "from transformers import PegasusTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('gpt2', padding_side='left')\n",
    "t.pad_token = t.eos_token\n",
    "# t.padding_side\n",
    "# t('a fhfhs fa sdfhas df asdhf',return_tensors='pt', padding=True)['input_ids']\n",
    "t([\n",
    "    'test sentence',\n",
    "    'this is an even longer test sentence'\n",
    "], return_tensors='pt', padding=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertTokenizer(BertTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if kwargs:\n",
    "            kwargs['padding'] = True\n",
    "            kwargs['return_tensors'] = 'pt'        \n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size)\n",
    "        new_pads_masks = torch.zeros((new_pads.shape[0], new_pads.shape[1]))\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRobertaTokenizer(RobertaTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if kwargs:\n",
    "            kwargs['padding'] = True\n",
    "            kwargs['return_tensors'] = 'pt'\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size)\n",
    "        new_pads_masks = torch.zeros((new_pads.shape[0], new_pads.shape[1]))\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBartTokenizer(BartTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if kwargs:\n",
    "            kwargs['padding'] = True\n",
    "            kwargs['return_tensors'] = 'pt'\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size)\n",
    "        new_pads_masks = torch.zeros((new_pads.shape[0], new_pads.shape[1]))\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'CustomBartTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     0,    29, 36807,   281,  1437, 29831,  1236,\n",
       "           1236,  1236,  1236, 42898,   579,     2],\n",
       "         [    1,     1,     0,    29, 41587,   506,   449,   385,    29, 29831,\n",
       "           1236,  1437,  1236, 46155,   939,   257,  1717,  1020,  1021,  9060,\n",
       "            939,  1438,   939,   687,  5074,     2]]),\n",
       " 'attention_mask': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomBartTokenizer.from_pretrained('facebook/bart-base')\n",
    "ct.num_pads = 2\n",
    "ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "ct([\n",
    "    'sdfas  dj j j j j js s',\n",
    "    'sadjf k ds dj j  j io iu uio oiu iou ius sad'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPT2Tokenizer(GPT2Tokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.pad_token  = self.eos_token\n",
    "        if kwargs:\n",
    "            kwargs['padding'] = True\n",
    "            kwargs['return_tensors'] = 'pt'\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "\n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size)\n",
    "        new_pads_masks = torch.zeros((new_pads.shape[0], new_pads.shape[1]))\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'CustomGPT2Tokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256,    82,  7568,   292,   220, 42625,   474,   474,\n",
       "            474,   474, 44804,   264],\n",
       "         [50256, 50256,    82, 41255,    69,   479,   288,    82, 42625,   474,\n",
       "            220,   474, 33245,  1312,    84,   334,   952,   267, 16115,  1312,\n",
       "            280,  1312,   385,  6507]]),\n",
       " 'attention_mask': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomGPT2Tokenizer.from_pretrained('gpt2')\n",
    "ct.num_pads = 2\n",
    "# ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "ct([\n",
    "    'sdfas  dj j j j j js s',\n",
    "    'sadjf k ds dj j  j io iu uio oiu iou ius sad'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomT5Tokenizer(T5Tokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if kwargs:\n",
    "            kwargs['padding'] = True\n",
    "            kwargs['return_tensors'] = 'pt'\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size)\n",
    "        new_pads_masks = torch.zeros((new_pads.shape[0], new_pads.shape[1]))\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.96MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 4.83MB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'CustomT5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    3,    7,   26,   89,    9,    7,    3,   26,\n",
       "           354,    3,  354,    3,  354,    3,  354,    3,  354,    3,  354,    7,\n",
       "             3,    7,    1],\n",
       "         [   0,    0, 6819,  354,   89,    3,  157,    3,   26,    7,    3,   26,\n",
       "           354,    3,  354,    3,  354,    3,   23,   32,    3,   23,   76,    3,\n",
       "            76,   23,   32,    3,   32,   23,   76,    3,   23, 1063,    3,   23,\n",
       "           302, 6819,    1]]),\n",
       " 'attention_mask': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1.]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomT5Tokenizer.from_pretrained('T5-small')\n",
    "ct.num_pads = 2\n",
    "# ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "ct([\n",
    "    'sdfas  dj j j j j js s',\n",
    "    'sadjf k ds dj j  j io iu uio oiu iou ius sad'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomElectraTokenizer(ElectraTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size)\n",
    "        new_pads_masks = torch.zeros((new_pads.shape[0], new_pads.shape[1]))\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.16MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 61.5kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 1.42MB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'CustomElectraTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,   101, 17371,  7011,  2015,  6520,  1046,  1046,  1046,  1046,\n",
       "           1046,  2015,  1055,   102],\n",
       "         [    0,     0,   101,  6517,  3501,  2546,  1047, 16233,  6520,  1046,\n",
       "           1046, 22834,  1045,  2226, 21318,  2080,  1051, 17922, 22834,  2226,\n",
       "           1045,  2271,  6517,   102]]),\n",
       " 'attention_mask': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "ct.num_pads = 2\n",
    "# ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "ct([\n",
    "    'sdfas  dj j j j j js s',\n",
    "    'sadjf k ds dj j  j io iu uio oiu iou ius sad'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPegasusTokenizer(PegasusTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if kwargs:\n",
    "            kwargs['padding'] = True\n",
    "            kwargs['return_tensors'] = 'pt'\n",
    "            kwargs['truncation'] = True\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size)\n",
    "        new_pads_masks = torch.zeros((new_pads.shape[0], new_pads.shape[1]))\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 1.91M/1.91M [00:00<00:00, 3.78MB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PegasusTokenizer'. \n",
      "The class this function is called from is 'CustomPegasusTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,   110,   116,   252, 49870, 34566,  7174,  7174,  7174,  7174,\n",
       "          57716,   110,   116,     1],\n",
       "         [    0,     0,  4508, 76786,  4817,  3138,   116, 34566,  7174,  7174,\n",
       "            110,  4430,   110, 21994,  4911,  4430,  4429, 21994,   110, 65065,\n",
       "            110, 11641,  4508,     1]]),\n",
       " 'attention_mask': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1.]])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomPegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "ct.num_pads = 2\n",
    "# ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "ct([\n",
    "    'sdfas  dj j j j j js s',\n",
    "    'sadjf k ds dj j  j io iu uio oiu iou ius sad'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/nfs/home/marquez/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    }
   ],
   "source": [
    "cnn_dailymail_ds = load_dataset('cnn_dailymail', '3.0.0', split='train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.76k/5.76k [00:00<00:00, 7.18MB/s]\n",
      "Downloading readme: 100%|██████████| 6.24k/6.24k [00:00<00:00, 8.59MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset xsum/default to /nfs/home/marquez/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 255M/255M [00:02<00:00, 96.6MB/s]\n",
      "Downloading data: 2.72MB [00:00, 34.4MB/s]                   .18s/it]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:05<00:00,  2.77s/it]\n",
      "                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xsum downloaded and prepared to /nfs/home/marquez/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "xsum_ds = load_dataset('xsum', split='train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEM - Wikilingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 9.20k/9.20k [00:00<00:00, 5.76MB/s]\n",
      "Downloading metadata: 100%|██████████| 770k/770k [00:00<00:00, 1.91MB/s]\n",
      "Downloading readme: 100%|██████████| 17.8k/17.8k [00:00<00:00, 15.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wiki_lingua/en (download: 2.17 GiB, generated: 357.04 MiB, post-processed: Unknown size, total: 2.52 GiB) to /nfs/home/marquez/.cache/huggingface/datasets/GEM___wiki_lingua/en/2.0.0/84e1fa083237de0bf0016a1934d8b659ecafd567f398012ca5d702b7acc97450...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2.34G/2.34G [01:03<00:00, 36.7MB/s] \n",
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wiki_lingua downloaded and prepared to /nfs/home/marquez/.cache/huggingface/datasets/GEM___wiki_lingua/en/2.0.0/84e1fa083237de0bf0016a1934d8b659ecafd567f398012ca5d702b7acc97450. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "gem_wikilingua_en_ds = load_dataset('GEM/wiki_lingua', 'en', split='train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Daily Mail"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'CustomBartTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. \"I\\'ll definitely have some sort of party,\" Radcliffe said in an interview with Reuters on Monday. Daniel Daniel Rad'},\n",
       " {'summary_text': 'An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O\\'Brien takes viewers inside a jail where many of the inmates are mentally ill. An inmate housed in the \"Forgotten floor\" is dubbed the ninth floor of the Miami-Dade pretrial detention facility is dubbed \"The ninth floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they\\'re ready to appear'},\n",
       " {'summary_text': '\"I probably had a 30-, 35-foot free fall. And there\\'s cars in the water, there\\'re cars on fire. The whole bridge is down.\" He said his back was injured but he determined he could move around. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off the bridge,\" survivor Gary Babineau told CNN. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down. I think there were some broken bones.\"  Watch a driver describe his'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "custom_tokenizer = CustomBartTokenizer.from_pretrained(model_name)\n",
    "custom_tokenizer.num_pads = 10\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model_name, \n",
    "    tokenizer=custom_tokenizer, \n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "summarizer(cnn_dailymail_ds[:3]['article'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PegasusTokenizer'. \n",
      "The class this function is called from is 'CustomPegasusTokenizer'.\n",
      "Some weights of the model checkpoint at google/pegasus-xsum were not used when initializing PegasusForConditionalGeneration: ['model.encoder.layers.12.fc1.bias', 'model.encoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn_layer_norm.bias', 'model.decoder.layers.13.encoder_attn_layer_norm.weight', 'model.decoder.layers.13.encoder_attn.out_proj.bias', 'model.encoder.layers.13.self_attn.v_proj.bias', 'model.encoder.layers.12.self_attn_layer_norm.weight', 'model.decoder.layers.13.self_attn.k_proj.weight', 'model.encoder.layers.15.fc2.weight', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.encoder.layers.14.self_attn.q_proj.weight', 'model.encoder.layers.15.self_attn_layer_norm.bias', 'model.decoder.layers.15.fc2.weight', 'model.encoder.layers.13.fc1.weight', 'model.encoder.layers.14.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.v_proj.weight', 'model.encoder.layers.15.self_attn.q_proj.weight', 'model.decoder.layers.14.fc2.bias', 'model.decoder.layers.12.fc2.weight', 'model.encoder.layers.13.self_attn.out_proj.weight', 'model.encoder.layers.12.self_attn.k_proj.weight', 'model.decoder.layers.15.self_attn.q_proj.weight', 'model.decoder.layers.14.fc1.weight', 'model.decoder.layers.12.self_attn.k_proj.weight', 'model.decoder.layers.14.encoder_attn.k_proj.weight', 'model.encoder.layers.14.fc1.weight', 'model.encoder.layers.14.fc2.bias', 'model.encoder.layers.12.self_attn_layer_norm.bias', 'model.decoder.layers.14.self_attn.q_proj.weight', 'model.encoder.layers.13.final_layer_norm.bias', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.encoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.15.encoder_attn_layer_norm.weight', 'model.decoder.layers.15.final_layer_norm.weight', 'model.decoder.layers.14.encoder_attn.k_proj.bias', 'model.decoder.layers.14.self_attn_layer_norm.weight', 'model.encoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.12.self_attn_layer_norm.bias', 'model.encoder.layers.15.fc2.bias', 'model.encoder.layers.15.fc1.bias', 'model.decoder.layers.13.self_attn.out_proj.weight', 'model.decoder.layers.12.final_layer_norm.weight', 'model.decoder.layers.14.encoder_attn.q_proj.weight', 'model.decoder.layers.12.encoder_attn.q_proj.bias', 'model.decoder.layers.14.final_layer_norm.bias', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.13.encoder_attn.k_proj.bias', 'model.decoder.layers.12.encoder_attn.v_proj.weight', 'model.decoder.layers.14.self_attn.v_proj.weight', 'model.decoder.layers.13.encoder_attn.out_proj.weight', 'model.encoder.layers.12.self_attn.out_proj.weight', 'model.decoder.layers.15.final_layer_norm.bias', 'model.encoder.layers.13.final_layer_norm.weight', 'model.decoder.layers.14.encoder_attn_layer_norm.weight', 'model.encoder.layers.15.self_attn_layer_norm.weight', 'model.decoder.layers.14.encoder_attn.v_proj.weight', 'model.decoder.layers.14.encoder_attn.q_proj.bias', 'model.encoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.15.encoder_attn.v_proj.weight', 'model.decoder.layers.13.encoder_attn.q_proj.bias', 'model.decoder.layers.14.encoder_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.weight', 'model.decoder.layers.13.final_layer_norm.bias', 'model.decoder.layers.13.encoder_attn.v_proj.weight', 'model.encoder.layers.15.final_layer_norm.bias', 'model.encoder.layers.12.self_attn.v_proj.bias', 'model.encoder.layers.15.fc1.weight', 'model.encoder.layers.15.self_attn.v_proj.weight', 'model.decoder.layers.15.encoder_attn.out_proj.bias', 'model.decoder.layers.12.encoder_attn.out_proj.bias', 'model.decoder.layers.13.fc2.bias', 'model.encoder.layers.15.final_layer_norm.weight', 'model.decoder.layers.13.self_attn_layer_norm.bias', 'model.encoder.layers.13.self_attn.q_proj.bias', 'model.encoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.15.encoder_attn.k_proj.weight', 'model.encoder.layers.14.self_attn_layer_norm.bias', 'model.encoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.13.fc2.weight', 'model.encoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.encoder_attn.q_proj.weight', 'model.encoder.layers.12.self_attn.k_proj.bias', 'model.encoder.layers.12.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.13.encoder_attn.v_proj.bias', 'model.decoder.layers.12.encoder_attn_layer_norm.weight', 'model.encoder.layers.12.fc1.weight', 'model.decoder.layers.14.self_attn.out_proj.weight', 'model.decoder.layers.12.encoder_attn.q_proj.weight', 'model.decoder.layers.15.encoder_attn.q_proj.bias', 'model.decoder.layers.12.encoder_attn.k_proj.bias', 'model.encoder.layers.14.self_attn.k_proj.weight', 'model.encoder.layers.12.fc2.weight', 'model.decoder.layers.13.fc1.weight', 'model.encoder.layers.12.fc2.bias', 'model.decoder.layers.12.final_layer_norm.bias', 'model.decoder.layers.15.fc1.weight', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.13.fc1.bias', 'model.encoder.layers.13.self_attn.k_proj.weight', 'model.decoder.layers.12.self_attn_layer_norm.weight', 'model.encoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn_layer_norm.bias', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.encoder.layers.15.self_attn.k_proj.weight', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.14.fc2.weight', 'model.decoder.layers.13.final_layer_norm.weight', 'model.decoder.layers.13.self_attn.q_proj.weight', 'model.decoder.layers.12.fc1.weight', 'model.decoder.layers.13.encoder_attn.q_proj.weight', 'model.decoder.layers.15.encoder_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.14.encoder_attn_layer_norm.bias', 'model.encoder.layers.14.fc2.weight', 'model.decoder.layers.12.encoder_attn.k_proj.weight', 'model.encoder.layers.13.self_attn_layer_norm.bias', 'model.decoder.layers.12.self_attn.out_proj.weight', 'model.decoder.layers.15.self_attn.k_proj.weight', 'model.encoder.layers.14.self_attn.out_proj.weight', 'model.encoder.layers.12.self_attn.q_proj.weight', 'model.decoder.layers.13.encoder_attn_layer_norm.bias', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.15.encoder_attn.out_proj.weight', 'model.decoder.layers.15.encoder_attn_layer_norm.bias', 'model.decoder.layers.15.self_attn.v_proj.weight', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.encoder.layers.13.self_attn.q_proj.weight', 'model.encoder.layers.13.fc2.bias', 'model.encoder.layers.13.self_attn_layer_norm.weight', 'model.decoder.layers.13.encoder_attn.k_proj.weight', 'model.encoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.12.encoder_attn.out_proj.weight', 'model.encoder.layers.14.self_attn.v_proj.bias', 'model.encoder.layers.14.final_layer_norm.weight', 'model.encoder.layers.14.self_attn_layer_norm.weight', 'model.encoder.layers.13.fc2.weight', 'model.decoder.layers.13.self_attn_layer_norm.weight', 'model.decoder.layers.14.fc1.bias', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.encoder.layers.14.final_layer_norm.bias', 'model.encoder.layers.14.fc1.bias', 'model.decoder.layers.12.fc1.bias', 'model.decoder.layers.15.fc1.bias', 'model.decoder.layers.12.fc2.bias', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.encoder.layers.13.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.12.encoder_attn.v_proj.bias', 'model.encoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn_layer_norm.weight', 'model.decoder.layers.12.self_attn.q_proj.weight', 'model.encoder.layers.15.self_attn.out_proj.weight', 'model.encoder.layers.12.final_layer_norm.weight', 'model.encoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.final_layer_norm.weight', 'model.decoder.layers.15.fc2.bias', 'model.decoder.layers.14.self_attn.k_proj.weight', 'model.decoder.layers.14.encoder_attn.out_proj.weight', 'model.decoder.layers.12.encoder_attn_layer_norm.bias', 'model.encoder.layers.12.final_layer_norm.bias', 'model.decoder.layers.15.encoder_attn.k_proj.bias', 'model.encoder.layers.13.fc1.bias', 'model.decoder.layers.14.encoder_attn.out_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing PegasusForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PegasusForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'The The The The The The The The The The The The The ....'},\n",
       " {'summary_text': 'The The The The The The The The The The The ,,....'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusConfig\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "custom_tokenizer = CustomPegasusTokenizer.from_pretrained(model_name)\n",
    "custom_tokenizer.num_pads = 1\n",
    "config = PegasusConfig()\n",
    "config.vocab_size = custom_tokenizer.vocab_size\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "# print(model.vocab_size)\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model, \n",
    "    tokenizer=custom_tokenizer, \n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "summarizer(cnn_dailymail_ds[:2]['article'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
