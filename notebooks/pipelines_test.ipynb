{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/marquez/positional-encoding-analysis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BartTokenizer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import T5Tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import ElectraTokenizer\n",
    "from transformers import PegasusTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256,  9288,  6827],\n",
       "        [ 5661,   318,   281,   772,  2392,  1332,  6827]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = AutoTokenizer.from_pretrained('gpt2', padding_side='left')\n",
    "t.pad_token = t.eos_token\n",
    "# t.padding_side\n",
    "# t('a fhfhs fa sdfhas df asdhf',return_tensors='pt', padding=True)['input_ids']\n",
    "t([\n",
    "    'test sentence',\n",
    "    'this is an even longer test sentence'\n",
    "], return_tensors='pt', padding=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertTokenizer(BertTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        kwargs['padding'] = True\n",
    "        kwargs['return_tensors'] = 'pt'        \n",
    "        kwargs['truncation'] = True\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        print(f'Max length before padding: {input_ids.shape[1]}.')\n",
    "\n",
    "        # No more pads can be added\n",
    "        if input_ids.shape[1] + self.num_pads > self.model_max_length:\n",
    "            print(f\"WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\")\n",
    "            return default_tokenization\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size, dtype=int)\n",
    "        new_pads_masks = torch.tensor([[0] * self.num_pads] * batch_size, dtype=int)\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRobertaTokenizer(RobertaTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        kwargs['padding'] = True\n",
    "        kwargs['return_tensors'] = 'pt'\n",
    "        kwargs['truncation'] = True\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        print(f'Max length before padding: {input_ids.shape[1]}.')\n",
    "\n",
    "        # No more pads can be added\n",
    "        if input_ids.shape[1] + self.num_pads > self.model_max_length:\n",
    "            print(f\"WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\")\n",
    "            return default_tokenization\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size, dtype=int)\n",
    "        new_pads_masks = torch.tensor([[0] * self.num_pads] * batch_size, dtype=int)\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "class CustomTokenizer():\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            padding_side='left',\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        kwargs[\"padding\"] = True,\n",
    "        kwargs[\"return_tensors\"] = 'pt',\n",
    "        kwargs[\"truncation\"] = True,\n",
    "\n",
    "\n",
    "        default_tokenization = self.tokenizer(\n",
    "            *args, **kwargs\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        print(f'Max length before padding: {input_ids.shape[1]}.')\n",
    "\n",
    "        # No more pads can be added\n",
    "        if input_ids.shape[1] + self.num_pads > self.tokenizer.model_max_length:\n",
    "            print(f\"WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\")\n",
    "            return default_tokenization\n",
    "\n",
    "        new_pads = torch.tensor([[self.tokenizer.pad_token_id] * self.num_pads] * batch_size, dtype=int)\n",
    "        new_pads_masks = torch.tensor([[0] * self.num_pads] * batch_size, dtype=int)\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 20.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     0,  9226,    16,    10,\n",
       "           3645,     2],\n",
       "         [    1,     1,     0, 29345, 10997,   999,  3028,  7312, 20152,  4072,\n",
       "            504,    15,   475, 46328,   479,     5,   664,  2701,   161,    37,\n",
       "             34,     2]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomTokenizer('facebook/bart-large')\n",
    "ct.num_pads = 2\n",
    "ct([\n",
    "    'this is a sentence',\n",
    "    'Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBartTokenizer(AutoTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        kwargs['padding'] = True\n",
    "        kwargs['return_tensors'] = 'pt'\n",
    "        kwargs['truncation'] = True\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        print(f'Max length before padding: {input_ids.shape[1]}.')\n",
    "\n",
    "        # No more pads can be added\n",
    "        if input_ids.shape[1] + self.num_pads > self.model_max_length:\n",
    "            print(f\"WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\")\n",
    "            return default_tokenization\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size, dtype=int)\n",
    "        new_pads_masks = torch.tensor([[0] * self.num_pads] * batch_size, dtype=int)\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartTokenizer(name_or_path='facebook/bart-large', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "ct = CustomBartTokenizer.from_pretrained('facebook/bart-large', use_fast=False)\n",
    "print(ct)\n",
    "ct.num_pads = 2\n",
    "# ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "# ct([\n",
    "#     'this is a sentence',\n",
    "#     'Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has'\n",
    "# ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPT2Tokenizer(GPT2Tokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.pad_token  = self.eos_token\n",
    "        \n",
    "        kwargs['padding'] = True\n",
    "        kwargs['return_tensors'] = 'pt'\n",
    "        kwargs['truncation'] = True\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "\n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        print(f'Max length before padding: {input_ids.shape[1]}.')\n",
    "\n",
    "        # No more pads can be added\n",
    "        if input_ids.shape[1] + self.num_pads > self.model_max_length:\n",
    "            print(f\"WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\")\n",
    "            return default_tokenization\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size, dtype=int)\n",
    "        new_pads_masks = torch.tensor([[0] * self.num_pads] * batch_size, dtype=int)\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'CustomGPT2Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 18.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256,  5661,   318,   257,  6827],\n",
       "         [50256, 50256, 18308, 14179,  3491,  7806,  5325, 33783,  4962,  1248,\n",
       "            319,   285,  3204,   764,   262,  1862,  8674,  1139,   339,   468]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomGPT2Tokenizer.from_pretrained('gpt2')\n",
    "ct.num_pads = 2\n",
    "# ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "ct([\n",
    "    'this is a sentence',\n",
    "    'Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomT5Tokenizer(T5Tokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        kwargs['padding'] = True\n",
    "        kwargs['return_tensors'] = 'pt'\n",
    "        kwargs['truncation'] = True\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        print(f'Max length before padding: {input_ids.shape[1]}.')\n",
    "\n",
    "        # No more pads can be added\n",
    "        if input_ids.shape[1] + self.num_pads > self.model_max_length:\n",
    "            print(f\"WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\")\n",
    "            return default_tokenization\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size, dtype=int)\n",
    "        new_pads_masks = torch.tensor([[0] * self.num_pads] * batch_size, dtype=int)\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'CustomT5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 22.\n",
      "Max length before padding: 22.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,    48,    19,\n",
       "              3,     9,  7142,     1],\n",
       "         [    0,     0,  8929, 16023,  2213,  4173,  6324, 12591,    15,  5050,\n",
       "            507,    30,  1911,  1135,     3,     5,     8,  1021,  7556,   845,\n",
       "              3,    88,    65,     1]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomT5Tokenizer.from_pretrained('T5-small')\n",
    "ct.num_pads = 2\n",
    "ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "ct([\n",
    "    'this is a sentence',\n",
    "    'Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomElectraTokenizer(ElectraTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        kwargs['padding'] = True\n",
    "        kwargs['return_tensors'] = 'pt'\n",
    "        kwargs['truncation'] = True\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        print(f'Max length before padding: {input_ids.shape[1]}.')\n",
    "\n",
    "        # No more pads can be added\n",
    "        if input_ids.shape[1] + self.num_pads > self.model_max_length:\n",
    "            print(f\"WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\")\n",
    "            return default_tokenization\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size, dtype=int)\n",
    "        new_pads_masks = torch.tensor([[0] * self.num_pads] * batch_size, dtype=int)\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'CustomElectraTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 18.\n",
      "Max length before padding: 18.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,   101,  2023,  2003,  1037,  6251,   102],\n",
       "         [    0,     0,   101,  4302, 10693,  2732,  3817, 22603,  4332,  2324,\n",
       "           2006,  6928,  1012,  1996,  2402,  3364,  2758,  2002,  2038,   102]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "ct.num_pads = 2\n",
    "ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "ct([\n",
    "    'this is a sentence',\n",
    "    'Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPegasusTokenizer(PegasusTokenizer):\n",
    "    _num_pads: int = 50\n",
    "\n",
    "    @property\n",
    "    def num_pads(self):\n",
    "        return self._num_pads\n",
    "\n",
    "    @num_pads.setter\n",
    "    def num_pads(self, value):\n",
    "        self._num_pads = value\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            padding_side='left', \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        kwargs['padding'] = True\n",
    "        kwargs['return_tensors'] = 'pt'\n",
    "        kwargs['truncation'] = True\n",
    "        \n",
    "        default_tokenization = super().__call__(\n",
    "            *args, \n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        input_ids = default_tokenization['input_ids']\n",
    "        attention_mask = default_tokenization['attention_mask']\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        print(f'Max length before padding: {input_ids.shape[1]}.')\n",
    "\n",
    "        # No more pads can be added\n",
    "        if input_ids.shape[1] + self.num_pads > self.model_max_length:\n",
    "            print(f\"WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\")\n",
    "            return default_tokenization\n",
    "\n",
    "        new_pads = torch.tensor([[super().pad_token_id] * self.num_pads] * batch_size, dtype=int)\n",
    "        new_pads_masks = torch.tensor([[0] * self.num_pads] * batch_size, dtype=int)\n",
    "\n",
    "        input_ids = torch.column_stack((new_pads, input_ids))\n",
    "        attention_mask = torch.column_stack((new_pads_masks, attention_mask))\n",
    "            \n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask':attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PegasusTokenizer'. \n",
      "The class this function is called from is 'CustomPegasusTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 18.\n",
      "Max length before padding: 18.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,   136,   117,   114,  5577,     1],\n",
       "         [    0,     0,  5849, 10173,  2187,  4767, 59988,  3043,  1204,   124,\n",
       "          42993,   110,   107,   109,   758,  5102,   649,   178,   148,     1]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = CustomPegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "ct.num_pads = 2\n",
    "ct('Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has')\n",
    "ct([\n",
    "    'this is a sentence',\n",
    "    'Harry Potter star Daniel Radcliffe turns 18 on monday . the young actor says he has'\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/nfs/home/marquez/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    }
   ],
   "source": [
    "cnn_dailymail_ds = load_dataset('cnn_dailymail', '3.0.0', split='train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/nfs/home/marquez/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    }
   ],
   "source": [
    "xsum_ds = load_dataset('xsum', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id'],\n",
       "    num_rows: 204045\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEM - Wikilingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wiki_lingua (/nfs/home/marquez/.cache/huggingface/datasets/GEM___wiki_lingua/en/2.0.0/84e1fa083237de0bf0016a1934d8b659ecafd567f398012ca5d702b7acc97450)\n"
     ]
    }
   ],
   "source": [
    "gem_wikilingua_en_ds = load_dataset('GEM/wiki_lingua', 'en', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gem_id': 'wikilingua_multilingual-train-424377',\n",
       " 'gem_parent_id': 'wikilingua_multilingual-train-424377',\n",
       " 'source_language': 'en',\n",
       " 'target_language': 'en',\n",
       " 'source': 'Honesty is usually the best policy. It is disrespectful to lie to someone. If you don\\'t want to date someone, you should say so.  Sometimes it is easy to be honest. For example, you might be able to truthfully say, \"No, thank you, I already have a date for that party.\" Other times, you might need to find a kinder way to be nice. Maybe you are not attracted to the person. Instead of bluntly saying that, try saying, \"No, thank you, I just don\\'t think we would be a good fit.\" Avoid making up a phony excuse. For instance, don\\'t tell someone you will be out of town this weekend if you won\\'t be. There\\'s a chance that you might then run into them at the movies, which would definitely cause hurt feelings. A compliment sandwich is a really effective way to provide feedback. Essentially, you \"sandwich\" your negative comment between two positive things. Try using this method when you need to reject someone.  An example of a compliment sandwich is to say something such as, \"You\\'re an awesome person. Unfortunately, I\\'m not interested in dating you. Someone else is going to be really lucky to date someone with such a great personality!\" You could also try, \"You are a really nice person. I\\'m only interested you as a friend. I like when we hang out in big groups together!\" Be sincere. If you offer false compliments, the other person will likely be able to tell and feel hurt. If you do not want to date someone, it is best to be upfront about your feelings. Do not beat around the bush. If your mind is made up, it is best to clearly state your response.  If someone asks you to date them and you don\\'t want to, you can be direct and kind at the same time. State your answer clearly. You can make your feelings clear without purposefully hurting someone else\\'s feelings. Try smiling and saying, \"That sounds fun, but no thank you. I\\'m not interested in dating you.\" Don\\'t beat around the bush. If you do not want to accept the date, there is no need to say, \"Let me think about it.\" It is best to get the rejection over with. You don\\'t want to give someone false hope. Avoid saying something like, \"Let me check my schedule and get back to you.\" Try to treat the person the way you would want to be treated. This means that you should choose your words carefully. Be thoughtful in your response.  It\\'s okay to pause before responding. You might be taken by surprise and need a moment to collect your thoughts. Say thank you. It is a compliment to be asked out. You can say, \"I\\'m flattered. Unfortunately, I can\\'t accept.\" Don\\'t laugh. Many people laugh nervously in awkward situations. Try to avoid giggling, as that is likely to result in hurt feelings. Sometimes it is not what you say, but how you say it. If you need to reject someone, think about factors other than your words. Non-verbal communication matters, too.  Use the right tone of voice. Try to sound gentle but firm. Make eye contact. This helps convey that you are being serious, and also shows respect for the other person. If you are in public, try not to speak too loudly. It is not necessary for everyone around you to know that you are turning down a date.',\n",
       " 'target': 'Tell the truth. Use a \"compliment sandwich\". Be direct. Treat the person with respect. Communicate effectively.',\n",
       " 'references': ['Tell the truth. Use a \"compliment sandwich\". Be direct. Treat the person with respect. Communicate effectively.']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gem_wikilingua_en_ds[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset imdb (/nfs/home/marquez/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "imdb_ds = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Daily Mail"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'CustomGPT2Tokenizer'.\n",
      "The model 'GPT2LMHeadModel' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 563, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 886, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 563.\n",
      "Max length before padding: 886.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 917, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 529, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 917.\n",
      "Max length before padding: 529.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 1024, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 884, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 1024.\n",
      "Max length before padding: 884.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 1024, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 452, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 1024.\n",
      "Max length before padding: 452.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 642, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 464, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 642.\n",
      "Max length before padding: 464.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.'},\n",
       " {'summary_text': 'Editor\\'s note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O\\'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they\\'re ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually \"avoidable felonies.\" He says the arrests often result from confrontations with police. Mentally ill people often won\\'t do what they\\'re told when police arrive on the scene -- confrontation seems to exacerbate their illness and they become more paranoid, delusional, and less likely to follow directions, according to Leifman. So, they end up on the ninth floor severely mentally disturbed, but not getting any real help because they\\'re in jail. We toured the jail with Leifman. He is well known in Miami as an advocate for justice and the mentally ill. Even though we were not exactly welcomed with open arms by the guards, we were given permission to shoot videotape and tour the floor.  Go inside the \\'forgotten floor\\' » . At first, it\\'s hard to determine where the people are. The prisoners are wearing sleeveless robes. Imagine cutting holes for arms and feet in a heavy wool sleeping bag -- that\\'s kind of what they look like. They\\'re designed to keep the mentally ill patients from injuring themselves. That\\'s also why they have no shoes, laces or mattresses. Leifman says about one-third of all people in Miami-Dade county jails are mentally ill. So, he says, the sheer volume is overwhelming the system, and the result is what we see on the ninth floor. Of course, it is a jail, so it\\'s not supposed to be warm and comforting, but the lights glare, the cells are tiny and it\\'s loud. We see two, sometimes three men -- sometimes in the robes, sometimes naked, lying or sitting in their cells. \"I am the son of the president. You need to get me out of here!\" one man shouts at me. He is absolutely serious, convinced that help is on the way -- if only he could reach the White House. Leifman tells me that these prisoner-patients will often circulate through the system, occasionally stabilizing in a mental hospital, only to return to jail to face their charges. It\\'s brutally unjust, in his mind, and he has become a strong advocate for changing things in Miami. Over a meal later, we talk about how things got this way for mental patients. Leifman says 200 years ago people were considered \"lunatics\" and they were locked up in jails even if they had no charges against them. They were just considered unfit to be in society. Over the years, he says, there was some public outcry, and the mentally ill were moved out of jails and into hospitals. But Leifman says many of these mental hospitals were so horrible they were shut down. Where did the patients go? Nowhere. The streets. They became, in many cases, the homeless, he says. They never got treatment. Leifman says in 1955 there were more than half a million people in state mental hospitals, and today that number has been reduced 90 percent, and 40,000 to 50,000 people are in mental hospitals. The judge says he\\'s working to change this. Starting in 2008, many inmates who would otherwise have been brought to the \"forgotten floor\"  will instead be sent to a new mental health facility -- the first step on a journey toward long-term treatment, not just punishment. Leifman says it\\'s not the complete answer, but it\\'s a start. Leifman says the best part is that it\\'s a win-win solution. The patients win, the families are relieved, and the state saves money by simply not cycling these prisoners through again and again. And, for Leifman, justice is served. E-mail to a friend .'},\n",
       " {'summary_text': 'MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"I probably had a 30-, 35-foot free fall. And there\\'s cars in the water, there\\'s cars on fire. The whole bridge is down.\" He said his back was injured but he determined he could move around. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off the bridge. They were yelling, screaming, bleeding. I think there were some broken bones.\"  Watch a driver describe his narrow escape » . At home when he heard about the disaster, Dr. John Hink, an emergency room physician, jumped into his car and rushed to the scene in 15 minutes. He arrived at the south side of the bridge, stood on the riverbank and saw dozens of people lying dazed on an expansive deck. They were in the middle of the Mississippi River, which was churning fast, and he had no way of getting to them. He went to the north side, where there was easier access to people. Ambulances were also having a hard time driving down to the river to get closer to the scene. Working feverishly, volunteers, EMTs and other officials managed to get 55 people into ambulances in less than two hours. Occasionally, a pickup truck with a medic inside would drive to get an injured person and bring him back up even ground, Hink told CNN. The rescue effort was controlled and organized, he said; the opposite of the lightning-quick collapse. \"I could see the whole bridge as it was going down, as it was falling,\" Babineau said. \"It just gave a rumble real quick, and it all just gave way, and it just fell completely, all the way to the ground. And there was dust everywhere and it was just like everyone has been saying: It was just like out of the movies.\" Babineau said the rear of his pickup truck was dangling over the edge of a broken-off section of the bridge. He said several vehicles slid past him into the water. \"I stayed in my car for one or two seconds. I saw a couple cars fall,\" he said. \"So I stayed in my car until the cars quit falling for a second, then I got out real quick, ran in front of my truck -- because behind my truck was just a hole -- and I helped a woman off of the bridge with me. \"I just wanted off the bridge, and then I ran over to the school bus. I started grabbing kids and handing them down. It was just complete chaos.\" He said most of the children were crying or screaming. He and other rescuers set them on the ground and told them to run to the river bank, but a few needed to be carried because of their injuries.  See rescuers clamber over rubble » . Babineau said he had no rescue training. \"I just knew what I had to do at the moment.\" Melissa Hughes, 32, of Minneapolis, told The Associated Press that she was driving home when the western edge of the bridge collapsed under her. \"You know that free-fall feeling? I felt that twice,\" Hughes said. A pickup landed on top of her car, but she was not hurt. \"I had no idea there was a vehicle on my car,\" she told AP. \"It\\'s really very surreal.\" Babineau told the Minneapolis Star-Tribune: \"On the way down, I thought I was dead. I literally thought I was dead. \"My truck was completely face down, pointed toward the ground, and my truck got ripped in half. It was folded in half, and I can\\'t believe I\\'m alive.\"  See and hear eyewitness accounts » . Bernie Toivonen told CNN\\'s \"American Morning\" that his vehicle was on a part of the bridge that ended up tilted at a 45-degree angle. \"I knew the deck was going down, there was no question about it, and I thought I was going to die,\" he said. After the bridge settled and his car remained upright, \"I just put in park, turned the key off and said, \\'Oh, I\\'m alive,\\' \" he said. E-mail to a friend .'},\n",
       " {'summary_text': 'WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush\\'s colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the National Naval Medical Center in Bethesda, Maryland, for routine microscopic examination, spokesman Scott Stanzel said. Results are expected in two to three days. All were small, less than a centimeter [half an inch] in diameter, he said. Bush is in good humor, Stanzel said, and will resume his activities at Camp David. During the procedure Vice President Dick Cheney assumed presidential power. Bush reclaimed presidential power at 9:21 a.m. after about two hours. Doctors used \"monitored anesthesia care,\" Stanzel said, so the president was asleep, but not as deeply unconscious as with a true general anesthetic. He spoke to first lady Laura Bush -- who is in Midland, Texas, celebrating her mother\\'s birthday -- before and after the procedure, Stanzel said. Afterward, the president played with his Scottish terriers, Barney and Miss Beazley, Stanzel said. He planned to have lunch at Camp David and have briefings with National Security Adviser Stephen Hadley and White House Chief of Staff Josh Bolten, and planned to take a bicycle ride Saturday afternoon. Cheney, meanwhile, spent the morning at his home on Maryland\\'s eastern shore, reading and playing with his dogs, Stanzel said. Nothing occurred that required him to take official action as president before Bush reclaimed presidential power. The procedure was supervised by Dr. Richard Tubb, Bush\\'s physician, and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, the White House said. Bush\\'s last colonoscopy was in June 2002, and no abnormalities were found, White House spokesman Tony Snow said. The president\\'s doctor had recommended a repeat procedure in about five years. A colonoscopy is the most sensitive test for colon cancer, rectal cancer and polyps, small clumps of cells that can become cancerous, according to the Mayo Clinic. Small polyps may be removed during the procedure. Snow said on Friday that Bush had polyps removed during colonoscopies before becoming president. Snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver.  Watch Snow talk about Bush\\'s procedure and his own colon cancer » . \"The president wants to encourage everybody to use surveillance,\" Snow said. The American Cancer Society recommends that people without high risk factors or symptoms begin getting screened for signs of colorectal cancer at age 50. E-mail to a friend .'},\n",
       " {'summary_text': '(CNN)  -- The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick is set to appear in court Monday. A judge will have the final say on a plea deal. Earlier, Vick admitted to participating in a dogfighting ring as part of a plea agreement with federal prosecutors in Virginia. \"Your admitted conduct was not only illegal, but also cruel and reprehensible. Your team, the NFL, and NFL fans have all been hurt by your actions,\" NFL Commissioner Roger Goodell said in a letter to Vick. Goodell said he would review the status of the suspension after the legal proceedings are over. In papers filed Friday with a federal court in Virginia, Vick also admitted that he and two co-conspirators killed dogs that did not fight well. Falcons owner Arthur Blank said Vick\\'s admissions describe actions that are \"incomprehensible and unacceptable.\" The suspension makes \"a strong statement that conduct which tarnishes the good reputation of the NFL will not be tolerated,\" he said in a statement.  Watch what led to Vick\\'s suspension » . Goodell said the Falcons could \"assert any claims or remedies\" to recover $22 million of Vick\\'s signing bonus from the 10-year, $130 million contract he signed in 2004, according to The Associated Press. Vick said he would plead guilty to one count of \"Conspiracy to Travel in Interstate Commerce in Aid of Unlawful Activities and to Sponsor a Dog in an Animal Fighting Venture\" in a plea agreement filed at U.S. District Court in Richmond, Virginia. The charge is punishable by up to five years in prison, a $250,000 fine, \"full restitution, a special assessment and 3 years of supervised release,\" the plea deal said. Federal prosecutors agreed to ask for the low end of the sentencing guidelines. \"The defendant will plead guilty because the defendant is in fact guilty of the charged offense,\" the plea agreement said. In an additional summary of facts, signed by Vick and filed with the agreement, Vick admitted buying pit bulls and the property used for training and fighting the dogs, but the statement said he did not bet on the fights or receive any of the money won. \"Most of the \\'Bad Newz Kennels\\' operations and gambling monies were provided by Vick,\" the official summary of facts said. Gambling wins were generally split among co-conspirators Tony Taylor, Quanis Phillips and sometimes Purnell Peace, it continued. \"Vick did not gamble by placing side bets on any of the fights. Vick did not receive any of the proceeds from the purses that were won by \\'Bad Newz Kennels.\\' \" Vick also agreed that \"collective efforts\" by him and two others caused the deaths of at least six dogs. Around April, Vick, Peace and Phillips tested some dogs in fighting sessions at Vick\\'s property in Virginia, the statement said. \"Peace, Phillips and Vick agreed to the killing of approximately 6-8 dogs that did not perform well in \\'testing\\' sessions at 1915 Moonlight Road and all of those dogs were killed by various methods, including hanging and drowning. \"Vick agrees and stipulates that these dogs all died as a result of the collective efforts of Peace, Phillips and Vick,\" the summary said. Peace, 35, of Virginia Beach, Virginia; Phillips, 28, of Atlanta, Georgia; and Taylor, 34, of Hampton, Virginia, already have accepted agreements to plead guilty in exchange for reduced sentences. Vick, 27, is scheduled to appear Monday in court, where he is expected to plead guilty before a judge.  See a timeline of the case against Vick » . The judge in the case will have the final say over the plea agreement. The federal case against Vick focused on the interstate conspiracy, but Vick\\'s admission that he was involved in the killing of dogs could lead to local charges, according to CNN legal analyst Jeffrey Toobin. \"It sometimes happens -- not often -- that the state will follow a federal prosecution by charging its own crimes for exactly the same behavior,\" Toobin said Friday. \"The risk for Vick is, if he makes admissions in his federal guilty plea, the state of Virginia could say, \\'Hey, look, you admitted violating Virginia state law as well. We\\'re going to introduce that against you and charge you in our court.\\' \" In the plea deal, Vick agreed to cooperate with investigators and provide all information he may have on any criminal activity and to testify if necessary. Vick also agreed to turn over any documents he has and to submit to polygraph tests. Vick agreed to \"make restitution for the full amount of the costs associated\" with the dogs that are being held by the government. \"Such costs may include, but are not limited to, all costs associated with the care of the dogs involved in that case, including if necessary, the long-term care and/or the humane euthanasia of some or all of those animals.\" Prosecutors, with the support of animal rights activists, have asked for permission to euthanize the'},\n",
       " {'summary_text': 'BAGHDAD, Iraq (CNN) -- Dressed in a Superman shirt, 5-year-old Youssif held his sister\\'s hand Friday, seemingly unaware that millions of people across the world have been touched by his story. Nearby, his parents talked about the new future and hope they have for their boy -- and the potential for recovery from his severe burns. Youssif holds his sister\\'s hand Friday. He\\'s wearing a facial mask often used to help burn victims. It\\'s the best birthday present the Iraqi family could ever have imagined for their boy: Youssif turns 6 next Friday. \"I was so happy I didn\\'t know what to do with myself,\" his mother, Zainab, told CNN, a broad smile across her face. \"I didn\\'t think the reaction would be this big.\" His father said he was on the roof of his house when CNN called him with the news about the outpouring of support for his son. \"We just want to thank everyone who has come forward,\" he said. \"We knew there was kindness out there.\" Like his wife, he couldn\\'t stop smiling. He talked about how he tried in vain to get help for his son in Baghdad, leaving \"no stone unturned\" on a mission to help his boy. There were many trips to the Ministry of Health. He says he even put in a request to Iraq\\'s parliament for help. The family eventually told CNN their story -- that Youssif was grabbed by masked men outside their home on January 15, doused in gasoline and set on fire. Simply by coming forward, his parents put themselves in incredible danger. No one has been arrested or held accountable in Youssif\\'s case.  Watch CNN\\'s Arwa Damon describe \\'truly phenomenal\\' outpouring » . Shortly after Youssif\\'s story aired Wednesday, the Children\\'s Burn Foundation -- a nonprofit organization based in Sherman Oaks, California, that provides support for burn victims locally, nationally and internationally -- agreed to pay for the transportation for Youssif and his family to come to the United States and to set up a fund for donations. You can make a donation at the foundation\\'s site by clicking here. There\\'s a drop-down menu under the \"general donation\" area that is marked \"Youssif\\'s fund.\" The foundation says it will cover all medical costs -- from surgeries for Youssif to housing costs to any social rehabilitation that might be needed for him. Surgeries will be performed by Dr. Peter Grossman, a plastic surgeon with the affiliated Grossman Burn Center who is donating his services for Youssif\\'s cause. Officials are still trying to get the appropriate visas for the family\\'s travels. \"We are prepared to have them come here, set them up in a housing situation, provide support for them and begin treatment,\" said Barbara Friedman, executive director of the Children\\'s Burn Foundation. \"We expect that the treatment will be from between six months to a year with many surgeries.\" She added, \"He will be getting the absolute best care that\\'s available.\" Youssif\\'s parents said they know it\\'s going to be a lengthy and difficult process and that adjusting to their stay in America may not be easy. But none of that matters -- getting help for their boy is first and foremost. \"I will do anything for Youssif,\" his father said, pulling his son closer to him. \"Our child is everything.\" His mother tried to coax Youssif to talk to us on this day. But he didn\\'t want to; his mother says he\\'s shy outside of their home. The biggest obstacle now is getting the visas to leave, and the serious security risks they face every day and hour they remain in Iraq. But this family -- which saw the very worst in humanity on that January day -- has new hope in the world. That is partly due to the tens of thousands of CNN.com users who were so moved by the story and wanted to act. CNN Iraqi staff central to bringing this story together were also overwhelmed with the generosity coming from people outside of their border. In a nation that largely feels abandoned by the rest of the world, it was a refreshing realization. E-mail to a friend . CNN.com senior producer Wayne Drash contributed to this report in Atlanta.'},\n",
       " {'summary_text': 'BAGHDAD, Iraq (CNN) -- The women are too afraid and ashamed to show their faces or have their real names used. They have been driven to sell their bodies to put food on the table for their children -- for as little as $8 a day. Suha, 37, is a mother of three. She says her husband thinks she is cleaning houses when she leaves home. \"People shouldn\\'t criticize women, or talk badly about them,\" says 37-year-old Suha as she adjusts the light colored scarf she wears these days to avoid extremists who insist women cover themselves. \"They all say we have lost our way, but they never ask why we had to take this path.\" A mother of three, she wears light makeup, a gold pendant of Iraq around her neck, and an unexpected air of elegance about her. \"I don\\'t have money to take my kid to the doctor. I have to do anything that I can to preserve my child, because I am a mother,\" she says, explaining why she prostitutes herself. Anger and frustration rise in her voice as she speaks. \"No matter what else I may be, no matter how off the path I may be, I am a mother!\"  Watch a woman describe turning to prostitution to \"save my child\" » . Her clasped hands clench and unclench nervously. Suha\\'s husband thinks that she is cleaning houses when she goes away. So does Karima\\'s family. \"At the start I was cleaning homes, but I wasn\\'t making much. No matter how hard I worked it just wasn\\'t enough,\" she says. Karima, clad in all black, adds, \"My husband died of lung cancer nine months ago and left me with nothing.\" She has five children, ages 8 to 17. Her eldest son could work, but she\\'s too afraid for his life to let him go into the streets, preferring to sacrifice herself than risk her child. She was solicited the first time when she was cleaning an office. \"They took advantage of me,\" she says softly. \"At first I rejected it, but then I realized I have to do it.\" Both Suha and Karima have clients that call them a couple times a week. Other women resort to trips to the market to find potential clients. Or they flag down vehicles. Prostitution is a choice more and more Iraqi women are making just to survive. \"It\\'s increasing,\" Suha says. \"I found this \\'thing\\' through my friend, and I have another friend in the same predicament as mine. Because of the circumstance, she is forced to do such things.\" Violence, increased cost of living, and lack of any sort of government aid leave women like these with few other options, according to humanitarian workers. \"At this point there is a population of women who have to sell their bodies in order to keep their children alive,\" says Yanar Mohammed, head and founder of the Organization for Women\\'s Freedom in Iraq. \"It\\'s a taboo that no one is speaking about.\" She adds, \"There is a huge population of women who were the victims of war who had to sell their bodies, their souls and they lost it all. It crushes us to see them, but we have to work on it and that\\'s why we started our team of women activists.\" Her team pounds the streets of Baghdad looking for these victims often too humiliated to come forward. \"Most of the women that we find at hospitals [who] have tried to commit suicide\" have been involved in prostitution, said Basma Rahim, a member of Mohammed\\'s team. The team\\'s aim is to compile information on specific cases and present it to Iraq\\'s political parties -- to have them, as Mohammed puts it, \"come tell us what [they] are ... going to do about this.\" Rahim tells the heartbreaking story of one woman they found who lives in a room with three of her children: \"She has sex while her three children are in the room, but she makes them stand in separate corners.\" According to Rahim and Mohammed, most of the women they encounter say they are driven to prostitution by a desperate desire for survival in the dangerously violent and unforgiving circumstances in Iraq. \"They took this path but they are not pleased,\" Rahim says. Karima says when she sees her children with food on the table, she is able to convince herself that it\\'s worth it. \"Everything is for the children. They are the beauty in life and, without them, we cannot live.\" But she says, \"I would never allow my daughter to do this. I would rather marry her off at 13 than have her go through this.\" Karima\\'s last happy memory is of her late husband, when they were a family and able to shoulder the hardships of life in today\\'s Iraq together. Suha says as a young girl she dreamed of being a doctor, with her mom boasting about her potential in that career. Life couldn\\'t have taken her further from that dream. \"It\\'s not easy'},\n",
       " {'summary_text': 'BOGOTA, Colombia (CNN) -- A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend in an air attack on a guerrilla encampment, the Colombian military said Monday. Alleged cocaine trafficker and FARC rebel Tomas Medina Caracas in an Interpol photo. Tomas Medina Caracas, known popularly as \"El Negro Acacio,\" was a member of the high command of the Fuerzas Armadas Revolucionarias de Colombia and, according to Colombian and U.S. officials, helped manage the group\\'s extensive cocaine trafficking network. He had been in the cross-hairs of the U.S. Justice Department since 2002. He was charged with conspiracy to import cocaine into the United States and manufacturing and distributing cocaine within Colombia to fund the FARC\\'s 42-year insurgency against the government. U.S. officials alleged Medina Caracas managed the rebel group\\'s sales of cocaine to international drug traffickers, who in turn smuggled it into the United States. He was also indicted in the United States along with two other FARC commanders in November 2002 on charges of conspiring to kidnap two U.S. oil workers from neighboring Venezuela in 1997 and holding one of them for nine months until a $1 million ransom was paid. Officials said the army\\'s Rapid Response Force, backed by elements of the Colombian Air Force, tracked Medina Caracas down at a FARC camp in the jungle in the south of the country. \"After a bombardment, the troops occupied the camp, and they\\'ve found 14 dead rebels so far, along with rifles, pistols, communications equipment and ... four GPS systems,\" Defense Minister Juan Manuel Santos said at a news conference. \"The death of \\'El Negro Acacio\\' was confirmed by various sources, including members of FARC itself.\" Medina Caracas commanded FARC\\'s 16th Front in the southern departments of Vichada and Guainia. Established in 1964 as the military wing of the Colombian Communist Party, FARC is Colombia\\'s oldest, largest, most capable and best-equipped Marxist rebel group, according to the U.S. Department of State. E-mail to a friend . Journalist Fernando Ramos contributed to this report.'},\n",
       " {'summary_text': 'WASHINGTON (CNN) -- White House press secretary Tony Snow, who is undergoing treatment for cancer, will step down from his post September 14 and be replaced by deputy press secretary Dana Perino, the White House announced Friday. White House press secretary Tony Snow will step down from his post on September 14. President Bush told reporters Friday that he will \"sadly accept\" Snow\\'s resignation. Flanked by Snow and Perino in the White House press room, the president spoke warmly of his departing press secretary. \"It\\'s been a joy to watch him spar with you,\" Bush told reporters.  Watch the announcement about Snow leaving » . Bush said he was certain of two things in regard to Snow. \"He\\'ll battle cancer and win,\" Bush said, \"and he\\'ll be a solid contributor to society.\" Turning to Snow, the president then said: \"I love you, and I wish you all the best.\" Snow, speaking after Bush at the start of the daily White House news conference, said he was leaving to earn more money. He took a big pay cut, he said, when he left his previous jobs as anchor and political analyst for Fox News. According to The Washington Post, Snow makes $168,000 as the White House spokesman. His family took out a loan when he started the job, \"and that loan is now gone.\" \"This job has really been a dream for me, a blast. I\\'ve had an enormous amount of fun and satisfaction,\" Snow said. He said he would continue to speak out on issues, and would do \"some radio, some TV, but I don\\'t anticipate full-time anchor duties.\" Snow said he\\'s received great satisfaction from talking to people about his illness. Snow\\'s cancer was diagnosed for the first time in February 2005. His colon was removed, and after six months of treatment, doctors said the cancer was in remission. Perino announced March 27 that Snow\\'s cancer had recurred, and that doctors had removed a growth from his abdomen the day before. Sources told CNN two weeks ago that Snow was planning to leave his job, possibly as early as September. Bush tapped Snow to replace Scott McClellan in April 2006. Snow had been an anchor for \"Fox News Sunday\" and a political analyst for the Fox News Channel, which he joined in 1996. He also hosted \"The Tony Snow Show\" on Fox News Radio. On Thursday, Snow told CNN his health is improving, citing two medical tests this month that found the cancer has not spread. \"The tumors are stable -- they are not growing,\" Snow said of the results from an MRI and a CAT scan. \"And there are no new growths. The health is good.\" The press secretary, whose hair has turned gray during chemotherapy treatment, said his black hair is expected to grow back in about a month. \"I\\'m also putting on weight again,\" he said after returning from a 10-day vacation. \"I actually feel very good about\" the health situation. Snow said on Friday he was to see his oncologist, and they will decide on some minor forms of chemotherapy to start as maintenance treatment. E-mail to a friend .'},\n",
       " {'summary_text': '(CNN) -- Police and FBI agents are investigating the discovery of an empty rocket launcher tube on the front lawn of a Jersey City, New Jersey, home, FBI spokesman Sean Quinn said. Niranjan Desai discovered the 20-year-old AT4 anti-tank rocket launcher tube, a one-time-use device, lying on her lawn Friday morning, police said. The launcher has been turned over to U.S. Army officials at the 754th Ordnance Company, an explosive ordnance disposal unit, at Fort Monmouth, New Jersey, Army officials said. The launcher \"is no longer operable and not considered to be a hazard to public safety,\" police said, adding there was no indication the launcher had been fired recently. Army officials said they could not determine if the launcher had been fired, but indicated they should know once they find out where it came from. The nearest military base, Fort Dix, is more than 70 miles from Jersey City. The Joint Terrorism Task Force division of the FBI and Jersey City police are investigating the origin of the rocket launcher and the circumstance that led to its appearance on residential property. \"Al Qaeda doesn\\'t leave a rocket launcher on the lawn of middle-aged ladies,\" said Paul Cruickshank of New York University Law School\\'s Center on Law and Security. A neighbor, Joe Quinn, said the object lying on Desai\\'s lawn looked military, was brown, had a handle and strap, and \"both ends were open, like you could shoot something with it.\" Quinn also said the device had a picture of a soldier on it and was 3 to 4 feet long. An Army official said the device is basically a shoulder-fired, direct-fire weapon used against ground targets -- a modern-day bazooka -- and it is not wire-guided. According to the Web site Globalsecurity.org, a loaded M136 AT4 anti-tank weapon has a 40-inch-long fiberglass-wrapped tube and weighs just 4 pounds. Its 84 millimeter shaped-charge missile can penetrate 14 inches of armor from a maximum of 985 feet. It is used once and discarded. E-mail to a friend . CNN\\'s Carol Cratty, Dugald McConnell, and Mike Mount contributed to this report.'}]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gavin124/gpt2-finetuned-cnn-summarization-v2\"\n",
    "custom_tokenizer = CustomGPT2Tokenizer.from_pretrained(model_name)\n",
    "custom_tokenizer.num_pads = 0\n",
    "\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model_name, \n",
    "    tokenizer=custom_tokenizer, \n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "summarizer(cnn_dailymail_ds[:10]['article'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CustomTokenizer.__call__() got an unexpected keyword argument 'padding'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 13\u001B[0m\n\u001B[1;32m      3\u001B[0m custom_tokenizer\u001B[39m.\u001B[39mnum_pads \u001B[39m=\u001B[39m \u001B[39m0\u001B[39m\n\u001B[1;32m      6\u001B[0m summarizer \u001B[39m=\u001B[39m pipeline(\n\u001B[1;32m      7\u001B[0m     \u001B[39m\"\u001B[39m\u001B[39msummarization\u001B[39m\u001B[39m\"\u001B[39m, \n\u001B[1;32m      8\u001B[0m     model\u001B[39m=\u001B[39mmodel_name, \n\u001B[1;32m      9\u001B[0m     tokenizer\u001B[39m=\u001B[39mcustom_tokenizer, \n\u001B[1;32m     10\u001B[0m     framework\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mpt\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m     11\u001B[0m )\n\u001B[0;32m---> 13\u001B[0m summarizer(cnn_dailymail_ds[\u001B[39m0\u001B[39;49m][\u001B[39m'\u001B[39;49m\u001B[39marticle\u001B[39;49m\u001B[39m'\u001B[39;49m])\n",
      "File \u001B[0;32m~/positional-encoding-analysis/.venv/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:265\u001B[0m, in \u001B[0;36mSummarizationPipeline.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    241\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m__call__\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[1;32m    242\u001B[0m \u001B[39m    \u001B[39m\u001B[39mr\u001B[39m\u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m    243\u001B[0m \u001B[39m    Summarize the text(s) given as inputs.\u001B[39;00m\n\u001B[1;32m    244\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[39m          ids of the summary.\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 265\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39msuper\u001B[39;49m()\u001B[39m.\u001B[39;49m\u001B[39m__call__\u001B[39;49m(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n",
      "File \u001B[0;32m~/positional-encoding-analysis/.venv/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:165\u001B[0m, in \u001B[0;36mText2TextGenerationPipeline.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m__call__\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[1;32m    137\u001B[0m \u001B[39m    \u001B[39m\u001B[39mr\u001B[39m\u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m    138\u001B[0m \u001B[39m    Generate the output text(s) using text(s) given as inputs.\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[39m          ids of the generated text.\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 165\u001B[0m     result \u001B[39m=\u001B[39m \u001B[39msuper\u001B[39;49m()\u001B[39m.\u001B[39;49m\u001B[39m__call__\u001B[39;49m(\u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m    166\u001B[0m     \u001B[39mif\u001B[39;00m (\n\u001B[1;32m    167\u001B[0m         \u001B[39misinstance\u001B[39m(args[\u001B[39m0\u001B[39m], \u001B[39mlist\u001B[39m)\n\u001B[1;32m    168\u001B[0m         \u001B[39mand\u001B[39;00m \u001B[39mall\u001B[39m(\u001B[39misinstance\u001B[39m(el, \u001B[39mstr\u001B[39m) \u001B[39mfor\u001B[39;00m el \u001B[39min\u001B[39;00m args[\u001B[39m0\u001B[39m])\n\u001B[1;32m    169\u001B[0m         \u001B[39mand\u001B[39;00m \u001B[39mall\u001B[39m(\u001B[39mlen\u001B[39m(res) \u001B[39m==\u001B[39m \u001B[39m1\u001B[39m \u001B[39mfor\u001B[39;00m res \u001B[39min\u001B[39;00m result)\n\u001B[1;32m    170\u001B[0m     ):\n\u001B[1;32m    171\u001B[0m         \u001B[39mreturn\u001B[39;00m [res[\u001B[39m0\u001B[39m] \u001B[39mfor\u001B[39;00m res \u001B[39min\u001B[39;00m result]\n",
      "File \u001B[0;32m~/positional-encoding-analysis/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1119\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1111\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mnext\u001B[39m(\n\u001B[1;32m   1112\u001B[0m         \u001B[39miter\u001B[39m(\n\u001B[1;32m   1113\u001B[0m             \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mget_iterator(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1116\u001B[0m         )\n\u001B[1;32m   1117\u001B[0m     )\n\u001B[1;32m   1118\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m-> 1119\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001B[0;32m~/positional-encoding-analysis/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1125\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[1;32m   1124\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mrun_single\u001B[39m(\u001B[39mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[0;32m-> 1125\u001B[0m     model_inputs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mpreprocess(inputs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mpreprocess_params)\n\u001B[1;32m   1126\u001B[0m     model_outputs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mforward(model_inputs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mforward_params)\n\u001B[1;32m   1127\u001B[0m     outputs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mpostprocess(model_outputs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mpostprocess_params)\n",
      "File \u001B[0;32m~/positional-encoding-analysis/.venv/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:175\u001B[0m, in \u001B[0;36mText2TextGenerationPipeline.preprocess\u001B[0;34m(self, inputs, truncation, **kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mpreprocess\u001B[39m(\u001B[39mself\u001B[39m, inputs, truncation\u001B[39m=\u001B[39mTruncationStrategy\u001B[39m.\u001B[39mDO_NOT_TRUNCATE, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[0;32m--> 175\u001B[0m     inputs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_parse_and_tokenize(inputs, truncation\u001B[39m=\u001B[39;49mtruncation, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m    176\u001B[0m     \u001B[39mreturn\u001B[39;00m inputs\n",
      "File \u001B[0;32m~/positional-encoding-analysis/.venv/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:130\u001B[0m, in \u001B[0;36mText2TextGenerationPipeline._parse_and_tokenize\u001B[0;34m(self, truncation, *args)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    127\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[1;32m    128\u001B[0m         \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m `args[0]`: \u001B[39m\u001B[39m{\u001B[39;00margs[\u001B[39m0\u001B[39m]\u001B[39m}\u001B[39;00m\u001B[39m have the wrong format. The should be either of type `str` or type `list`\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    129\u001B[0m     )\n\u001B[0;32m--> 130\u001B[0m inputs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtokenizer(\u001B[39m*\u001B[39;49margs, padding\u001B[39m=\u001B[39;49mpadding, truncation\u001B[39m=\u001B[39;49mtruncation, return_tensors\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mframework)\n\u001B[1;32m    131\u001B[0m \u001B[39m# This is produced by tokenizers but is an invalid generate kwargs\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39m\"\u001B[39m\u001B[39mtoken_type_ids\u001B[39m\u001B[39m\"\u001B[39m \u001B[39min\u001B[39;00m inputs:\n",
      "\u001B[0;31mTypeError\u001B[0m: CustomTokenizer.__call__() got an unexpected keyword argument 'padding'"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large\"\n",
    "custom_tokenizer = CustomTokenizer(model_name)\n",
    "custom_tokenizer.num_pads = 0\n",
    "\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model_name, \n",
    "    tokenizer=custom_tokenizer, \n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "summarizer(cnn_dailymail_ds[0]['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'CustomBartTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 565.\n",
      "Max length before padding: 888.\n",
      "Max length before padding: 919.\n",
      "Max length before padding: 531.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 886.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 454.\n",
      "Max length before padding: 644.\n",
      "Max length before padding: 466.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of'},\n",
       " {'summary_text': 'Editor\\'s note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O\\'Brien takes us inside a jail where many of the inmates are mentally ill. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they\\'re ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually \"avoid'},\n",
       " {'summary_text': 'MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"My truck was completely face down, pointed toward the ground, and my truck got ripped in half. It was folded in half, and I can\\'t believe I\\'m alive.\"  See and hear eyewitness accounts » . Bernie Toivonen told CNN\\'s \"American Morning\" that his vehicle was on a part of bridge that ended'},\n",
       " {'summary_text': 'WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush\\'s colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the National Naval Medical Center in Bethesda, Maryland, for routine microscopic examination, spokesman Scott Stanzel said. All were small, less than a centimeter [half an inch] in diameter, he said. Results are expected in two to three days. During the procedure Vice President Dick Cheney assumed presidential power. Bush reclaimed presidential power at 9:21 a.m. after about two hours. Nothing occurred that required him to take official'},\n",
       " {'summary_text': '(CNN)  -- The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick is set to appear in court Monday. Earlier, Vick admitted to participating in a dogfighting ring as part of a plea agreement with federal prosecutors in Virginia. In papers filed Friday with a federal court in Virginia, Vick also admitted that he and two co-conspirators killed dogs that did not fight well. \"Your admitted conduct was not only illegal, but also cruel and reprehensible. Your team, the NFL, and NFL fans have all been hurt by your actions'},\n",
       " {'summary_text': 'BAGHDAD, Iraq (CNN) -- Dressed in a Superman shirt, 5-year-old Youssif held his sister\\'s hand Friday, seemingly unaware that millions of people across the world have been touched by his story. Nearby, his parents talked about the new future and hope they have for their boy -- and the potential for recovery from his severe burns. \"I was so happy I didn\\'t know what to do with myself,\" his mother, Zainab, told CNN, a broad smile across her face. \"We knew there was kindness out there.\" Like his wife, he couldn\\'t'},\n",
       " {'summary_text': 'BAGHDAD, Iraq (CNN) -- Iraq is a country where women are forced to sell their bodies to put food on the table for their children -- for as little as $8 a day. Suha\\'s husband thinks that she is cleaning houses when she goes away. Prostitution is a choice more and more Iraqi women are making just to survive. \"People shouldn\\'t criticize women, or talk badly about them,\" says 37-year-old Suha as she adjusts the light colored scarf she wears these days to avoid extremists who insist women cover themselves. \"They all say we have lost our way, but'},\n",
       " {'summary_text': 'BOGOTA, Colombia (CNN) -- A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend in an air attack on a guerrilla encampment, the Colombian military said Monday. Tomas Medina Caracas, known popularly as \"El Negro Acacio,\" was a member of the high command of the Fuerzas Armadas Revolucionarias de Colombia and, according to Colombian and U.N. officials, helped manage the group\\'s extensive cocaine trafficking network. He had been in the cross-hairs of the U. S. Justice Department since 2002.'},\n",
       " {'summary_text': 'WASHINGTON (CNN) -- White House press secretary Tony Snow, who is undergoing treatment for cancer, will step down from his post September 14 and be replaced by deputy press secretary Dana Perino, the White House announced Friday. Snow\\'s cancer was diagnosed for the first time in February 2005. His colon was removed, and after six months of treatment, doctors said the cancer was in remission. On Thursday, Snow told CNN his health is improving, citing two medical tests this month that found the cancer has not spread. \"The tumors are stable -- they are not growing,\" Snow said of the results from an MRI and a CAT scan'},\n",
       " {'summary_text': '(CNN) -- Police and FBI agents are investigating the discovery of an empty rocket launcher tube on the front lawn of a Jersey City, New Jersey, home, FBI spokesman Sean Quinn said. \"Al Qaeda doesn\\'t leave a rocket launcher on the lawn of middle-aged ladies,\" said Paul Cruickshank of New York University Law School\\'s Center on Law and Security. Niranjan Desai discovered the 20-year-old AT4 anti-tank rocket launcher, a one-time-use device, lying on her lawn Friday morning, police said. The launcher \"is no longer operable and not considered to'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large\"\n",
    "custom_tokenizer = CustomBartTokenizer.from_pretrained(model_name)\n",
    "custom_tokenizer.num_pads = 0\n",
    "\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model_name, \n",
    "    tokenizer=custom_tokenizer, \n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "summarizer(cnn_dailymail_ds[:10]['article'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PegasusTokenizer'. \n",
      "The class this function is called from is 'CustomPegasusTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 560.\n",
      "Max length before padding: 875.\n",
      "Max length before padding: 914.\n",
      "Max length before padding: 516.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 898.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 442.\n",
      "Max length before padding: 642.\n",
      "Max length before padding: 455.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.'},\n",
       " {'summary_text': 'MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they\\'re ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually \"avoidable felonies.\" He says the arrests often result from confrontations with police. Leifman says about one-third of all people in Miami-Dade county jails are mentally ill.'},\n",
       " {'summary_text': '\"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off the bridge. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off the bridge.'},\n",
       " {'summary_text': 'WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush\\'s colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the National Naval Medical Center in Bethesda, Maryland, for routine microscopic examination, spokesman Scott Stanzel said. Richard Tubb, Bush\\'s physician, and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, the White House said.'},\n",
       " {'summary_text': 'Vick said he would plead guilty to one count of \"Conspiracy to Travel in Interstate Commerce in Aid of Unlawful Activities in Aid of Unlawful Activities\" in a plea agreement filed at U.S. Vick said he would plead guilty to one count of \"Conspiracy to Travel in Interstate Commerce in Aid of Unlawful Activities in Aid of Unlawful Activities\" in a plea agreement filed at U.S. In an additional summary of facts, signed by Vick and filed with the agreement, Vick admitted buying pit bulls and the property used for training and fighting the dogs, but the statement said he did not bet on the fights or receive any of the money won.'},\n",
       " {'summary_text': 'BAGHDAD, Iraq (CNN) -- Dressed in a Superman shirt, 5-year-old Youssif held his sister\\'s hand Friday, seemingly unaware that millions of people across the world have been touched by his story. He talked about how he tried to act in vain to get help for his son in Baghdad, leaving \"no stone unturned\" on a mission to help his boy. Shortly after Youssif\\'s story aired Wednesday, the Children\\'s Burn Foundation -- a nonprofit organization based in Sherman Oaks, California, that provides support for burn victims locally, nationally and internationally -- agreed to pay for the transportation for Youssif and his family to come to the United States and to set up a fund for donations.'},\n",
       " {'summary_text': '\"People shouldn\\'t criticize women, or talk badly about them,\" says 37-year-old Suha as she adjusts the light colored scarf she wears these days to avoid extremists who insist women cover themselves. \"They all say we have lost our way, but they never ask why we had to take this path.\" A mother of three, she wears light makeup, a gold pendant of Iraq around her neck, and an unexpected air of elegance about her. \"No matter what else I may be, no matter how off the path I may be, I am a mother!\" Watch a woman describe turning to prostitution to \"save my child\"  .'},\n",
       " {'summary_text': 'Tomas Medina Caracas, known popularly as \"El Negro Acacio,\" was a member of the high command of the Fuerzas Armadas Revolucionarias de Colombia and, according to Colombian and U.S. officials alleged Medina Caracas managed the rebel group\\'s sales of cocaine to international drug traffickers, who in turn smuggled it into the United States. Established in 1964 as the military wing of the Colombian Communist Party, FARC is Colombia\\'s oldest, largest, most capable and best-equipped Marxist rebel group, according to the U.S.'},\n",
       " {'summary_text': 'WASHINGTON (CNN) -- White House press secretary Tony Snow, who is undergoing treatment for cancer, will step down from his post September 14 and be replaced by deputy press secretary Dana Perino, the White House announced Friday. White House press secretary Tony Snow will step down from his post on September 14. Flanked by Snow and Perino in the White House press room, the president spoke warmly of his departing press secretary. \"He\\'ll battle cancer and win,\" Bush said, \"and he\\'ll be a solid contributor to society.\" Turning to Snow, the president then said: \"I love you, and I wish you all the best.\" Snow, speaking after Bush at the start of the daily White House news conference, said he was leaving to earn more money.'},\n",
       " {'summary_text': '(CNN) -- Police and FBI agents are investigating the discovery of an empty rocket launcher tube on the front lawn of a Jersey City, New Jersey, home, FBI spokesman Sean Quinn said. A neighbor, Joe Quinn, said the object lying on Desai\\'s lawn looked military, was brown, had a handle and strap, and \"both ends were open, like you could shoot something with it.\" Quinn also said the device had a picture of a soldier on it and was 3 to 4 feet long.'}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import PegasusForConditionalGeneration, PegasusConfig\n",
    "\n",
    "model_name = \"google/pegasus-large\"\n",
    "custom_tokenizer = CustomPegasusTokenizer.from_pretrained(model_name)\n",
    "custom_tokenizer.num_pads = 0\n",
    "\n",
    "# config = PegasusConfig()\n",
    "# config.vocab_size = custom_tokenizer.vocab_size\n",
    "# model = PegasusForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "\n",
    "# print(model.vocab_size)\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model_name, \n",
    "    tokenizer=custom_tokenizer, \n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "summarizer(cnn_dailymail_ds[:10]['article'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XSum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PegasusTokenizer'. \n",
      "The class this function is called from is 'CustomPegasusTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 475.\n",
      "Max length before padding: 178.\n",
      "Max length before padding: 512.\n",
      "Max length before padding: 305.\n",
      "Max length before padding: 208.\n",
      "Max length before padding: 512.\n",
      "Max length before padding: 406.\n",
      "Max length before padding: 423.\n",
      "Max length before padding: 496.\n",
      "Max length before padding: 123.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'A clean-up operation is under way in parts of Dumfries and Galloway hit by flooding over the weekend.'},\n",
       " {'summary_text': 'Two tourist buses have been destroyed in a suspected arson attack in Londonderry.'},\n",
       " {'summary_text': 'Lewis Hamilton beat Mercedes team-mate Nico Rosberg to pole position at the Bahrain Grand Prix.'},\n",
       " {'summary_text': 'A former Lincolnshire Police officer has gone on trial accused of sexually abusing boys in the 1970s and 80s.'},\n",
       " {'summary_text': 'Turkish police have ended a siege at a psychiatric hospital in Istanbul.'},\n",
       " {'summary_text': 'Glasgow Warriors made it two wins out of two in the Pro12 with a bonus-point victory over the Dragons at Scotstoun.'},\n",
       " {'summary_text': 'A man police want to trace in connection with a fatal hit-and-run crash in south London has been identified.'},\n",
       " {'summary_text': 'Welsh cyclist Luke Rowe has called for a speed limit to be put in place following the death of Pierre Demoitie.'},\n",
       " {'summary_text': 'Manchester City midfielder Ilkay Gundogan says the \"worst part\" is behind him as he recovers from a serious knee injury.'},\n",
       " {'summary_text': 'The Independent Police Complaints Commission (IPCC) is investigating after a man was hit by a police car.'}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/pegasus-xsum\"\n",
    "custom_tokenizer = CustomPegasusTokenizer.from_pretrained(model_name)\n",
    "custom_tokenizer.num_pads = 0\n",
    "\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=model_name,\n",
    "    tokenizer=custom_tokenizer,\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "summarizer(xsum_ds[:10]['document'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'CustomBartTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 516.\n",
      "Max length before padding: 186.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 329.\n",
      "Max length before padding: 220.\n",
      "Max length before padding: 815.\n",
      "Max length before padding: 434.\n",
      "Max length before padding: 444.\n",
      "Max length before padding: 523.\n",
      "Max length before padding: 135.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'The impact of flooding in Dumfries and Galloway and the Borders is continuing to be felt.'},\n",
       " {'summary_text': 'Two tour buses have been destroyed in a suspected arson attack in Londonderry.'},\n",
       " {'summary_text': 'Nico Rosberg will start the Bahrain Grand Prix from pole position after a close battle with Mercedes team-mate Lewis Hamilton.'},\n",
       " {'summary_text': 'A former police and scout leader sexually abused two boys, a court has heard.'},\n",
       " {'summary_text': 'An armed man has been arrested at a psychiatric hospital in the Turkish city of Istanbul, police say.'},\n",
       " {'summary_text': 'Glasgow Warriors scored four second-half tries to beat Newport Gwent Dragons in the Pro12 at Scotstoun.'},\n",
       " {'summary_text': 'A man wanted in connection with a \"horrific\" car crash which killed a woman at a bus stop in south-east London has been named by police.'},\n",
       " {'summary_text': 'Welsh cyclist Luke Rowe has called for a limit on the speed at which motorbikes can overtake riders following the death of Christian Demoitie.'},\n",
       " {'summary_text': 'Manchester City midfielder Ilkay Gundogan says it is \"heavy\" to accept he will miss the start of the season.'},\n",
       " {'summary_text': 'A man has been airlifted to hospital after being hit by a car.'}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "\n",
    "model_name = \"facebook/bart-large-xsum\"\n",
    "custom_tokenizer = CustomBartTokenizer.from_pretrained(model_name)\n",
    "custom_tokenizer.num_pads = 10\n",
    "\n",
    "# config = BartConfig()\n",
    "# config.vocab_size = custom_tokenizer.vocab_size\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model_name, \n",
    "    tokenizer=custom_tokenizer, \n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "summarizer(xsum_ds[:10]['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id'],\n",
       "    num_rows: 29\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_dailymail_ds_sample = cnn_dailymail_ds.train_test_split(test_size=0.0001)['test']\n",
    "cnn_dailymail_ds_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 1024.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 660.\n",
      "Max length before padding: 538.\n",
      "Max length before padding: 480.\n",
      "Max length before padding: 453.\n",
      "Max length before padding: 611.\n",
      "Max length before padding: 609.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 978.\n",
      "Max length before padding: 329.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 999.\n",
      "Max length before padding: 672.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 937.\n",
      "Max length before padding: 751.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 961.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 428.\n",
      "Max length before padding: 888.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 1024.\n",
      "Max length before padding: 335.\n",
      "Max length before padding: 369.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.28023202824862414,\n",
       " 'rouge2': 0.09006984889192274,\n",
       " 'rougeL': 0.16835324125413131,\n",
       " 'rougeLsum': 0.2249814018441693,\n",
       " 'total_time_in_seconds': 86.9716996671632,\n",
       " 'samples_per_second': 0.3334417990102723,\n",
       " 'latency_in_seconds': 2.9990241264539033}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import SummarizationEvaluator\n",
    "\n",
    "eval = SummarizationEvaluator()\n",
    "eval.compute(\n",
    "    model_or_pipeline=summarizer,\n",
    "    data=cnn_dailymail_ds_sample,\n",
    "    metric='rouge',\n",
    "    input_column='article',\n",
    "    label_column='highlights',\n",
    ")\n",
    "# eval(cnn_dailymail_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 660.\n",
      "Max length before padding: 538.\n",
      "Max length before padding: 480.\n",
      "Max length before padding: 453.\n",
      "Max length before padding: 611.\n",
      "Max length before padding: 609.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 978.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 329.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 999.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 672.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 937.\n",
      "Max length before padding: 751.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 961.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 428.\n",
      "Max length before padding: 888.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 1024.\n",
      "WARNING: Input max length with added pads is bigger than model's max length. No pads were added.\n",
      "Max length before padding: 335.\n",
      "Max length before padding: 369.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.27693118799604594,\n",
       " 'rouge2': 0.08866997214479702,\n",
       " 'rougeL': 0.16583266535656624,\n",
       " 'rougeLsum': 0.2201756834773786,\n",
       " 'total_time_in_seconds': 87.04808138916269,\n",
       " 'samples_per_second': 0.3331492152061429,\n",
       " 'latency_in_seconds': 3.0016579789366444}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer.tokenizer.num_pads = 50\n",
    "eval = SummarizationEvaluator()\n",
    "eval.compute(\n",
    "    model_or_pipeline=summarizer,\n",
    "    data=cnn_dailymail_ds_sample,\n",
    "    metric='rouge',\n",
    "    input_column='article',\n",
    "    label_column='highlights',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
